# 1. 선형회귀

- **독립변수들이 종속변수에 미치는 영향**을 추정할 수 있는 통계기법

- 독립변수의 개수가 하나이면 단순선형회귀

- 독립변수의 개수가 두 개 이상이면 다중선형회귀분석



## 단순회귀분석의 가정

- **등분산성** : 각 좌표에 대한 오차의 퍼짐 정도가 비슷(오차의 분산이 입력변수와 무관하게 일정)

- **독립성** : 각 오차들은 독립적임(방향성 없음)

- **정규성** : 오차가 정규형태를 가짐

- **선형성** : '선형' 회귀분석 (입력변수와 출력변수의 관계가 선형임)

- **비상관성** : 오차들끼리 상관이 없음

<img src="https://velog.velcdn.com/images/shmaxlee/post/d1618b42-f026-4e75-8289-25f076c412bf/image.png" title="" alt="" width="463">



## 결정계수

<img src="https://mblogthumb-phinf.pstatic.net/MjAyMDA4MDlfNzQg/MDAxNTk2OTc1OTE3MzUy.x7XA_SyYVRBvWNJQfsnsGvvaG_7jffn40ADyXL-NBFIg.SCwYa78-Gpm4jS-Rtgv8OiifVg5CqScGXwRchArMz2kg.PNG.tlrror9496/image.png?type=w800" title="" alt="" width="470">

- 잔차제곱합 : SST

- 오차제곱합 : SSR

- 회귀제곱합 : SSE
  
  - `SSE = 0, R² = 0`  👉  X가 Y를 완벽하게 설명함
  
  - SSE가 작을 수록 `R² = 1`에 가까워짐



### 다중선형회귀

- 2개 이상의 독립변수가 종속변수에 영향을 미치는가?

- 가설 설정
  
  - 대립가설 : 모든 독립변수의 기울기가 0은 아니다(= 적어도 1개의 독립변수는 0이 아님)
  
  - 각 변수에 대해 검정해야 함

- 기본 5개 선형회귀 가정사항에 '**다중공선성**'도 확인
  
  - 다중공선성 : 독립변수 간 상관관계를 가지는 문제 최소화



### 중요 변수 찾기

- A, B 변수가 각각 사용될 때 어느 것이 더 중요한가
  
  - 상관계수로 확인(Correlation Coefficient)

- A변수와 B변수가 C변수와 함께 사용될 때 어느 것이 더 중요한가? => 교호작용 고려
  
  - 특정 변수 포함 시, 결정계수(R²) 증가량 파악
  
  - PCC (Part Correlation Coefficient, 부분상관계수)
  
  - PrCC (Partial Correlation Coefficient, 편상관계수)





# 2. 로지스틱 회귀모델

- 로지스틱 회귀는 선형 회귀 방식을 응용해 분류에 적용

- 이진 분류 문제에 주로 로지스틱 회귀 사용

- 시그모이드 함수를 활용해 타깃값에 포함될 확률 예측

![로지스틱](https://velog.velcdn.com/images/73syjs/post/9ece82c2-0abb-4379-b0a7-74ed203eaf8d/image.png)



# 3. 결정 트리(Decision Tree)

- 분류와 회귀 문제에 모두 사용가능한 모델

- 장점
  
  - 해석이 용이함 : 구조에 의해 모델이 표현되어 해석이 쉬움
  
  - 새로운 자료에 모델 적합이 쉬움
  
  - 비모수적 모델 : 선형성, 정규성, 등분산성의 가정이 필요하지 않음
  
  - 순위만 분석에 영향을 주므로 이상치에 덜 민감

- 단점
  
  - 비연속성 : 연속형 변수를 비연속적인 값으로 취급하여 예측 오류가 클 가능성이 있음
  
  - 선형성 : 선형 또는 주 효과 모델에서와 같은 결과를 얻지 못함
  
  - 비안정성 : 분석용 자료에만 의존하여 새로운 자료의 예측에 불안정함





#### 1. 형성 과정

- 1단계 : 변수 선택 (종속변수와 관계 있는 독립 변수 선택)

- 2단계 : 성장
  
  - 적절한 분리기준과 정지규칙을 정하여 구조 작성
  
  - 분리 규칙 : 불순도 감소량을 가장 크게 하는 기준을 찾아서 분할
  
  - 분리 기준 : 불순도에 의해 측정
  
  -  정지 규칙 : 분리가 중단되고 노드가 끝노드가 되도록 깊이 지정

- 3단계 : 가지치기
  
  - 속성 제한 : 나무의 깊이(depth), 잎 노드(left node)의 최대 개수
  
  - 최소 표본 분리 : 한 노드에 들어있는 최소 데이터의 수 정함

- 4단계 : 타당성 평가
  
  - 이익, 위험, 비용 등을 고려하여 모델 평가
  
  - 검증 오차가 가장 작은 의사결정나무 평가

- 5단계 : 예측
  
  - 분류 또는 예측 수행



#### 2. 불순도

- 한 범주 안에서 서로 다른 데이터가 얼마나 섞여 있는지 확인

- 측정 기준
  
  - 지니지수 : 0.5에 가까울수록 불순도가 높고, 0에 가까울수록 불순도 낮음
  
  - 엔트로피 : 1에 가까울수록 불순도가 높으며, 0에 가까울수록 불순도 낮음
  
  - 정보이득 : 결정트리는 정보이득을 최대화하는 방향(지니지수 또는 엔트로피 최소화하는 방향)으로 노드 분할



#### 주요 파라미터(Scikit-Learn)

| 파라미터              | 설명                                                                                                                  |
| ----------------- | ------------------------------------------------------------------------------------------------------------------- |
| cirterion         | 분할 시, 사용할 불순도 측정 지표 (옵션 : gini, entropy)                                                                            |
| max_depth         | 트리의 최대 깊이, 미 지정 시, Leaf Node의 불순도가 0이 되거나 노드에 있는 데이터 수가 min_samples_split보다 작을 때까지 트리 깊이가 깊어짐                       |
| min_samples_split | 노드 분할을 위한 최소 데이터 개수 : 노드 내 데이터 개수가 이 값보다 작으면 더 이상 분할하지 않음<br/>- 정수형 : 최소 데이터 개수<br/>- 실수형 : 전체 데이터 대비 최소 데이터 개수의 비율 |
| min_samples_leaf  | 말단 노드가 되기 위한 최소 데이터 개수 : 분할 후 노드 내 데이터 개수가 이 값보다 작으면 더 이상 분할하지 않음<br/>- 정수형과 실수형(위와 동일)                             |
| max_features      | 분할에 사용할 피처 개수                                                                                                       |



# 4. 앙상블 학습

> * 다양한 모델이 내린 예측 결과 결합 
> 
> * 과적합(Overfitting) 문제에 대한 극복방법으로 고안
> 
> - 여러 알고리즘 구성 & 데이터를 다양하게 구성
> 
> - 전체적인 예측값의 분산을 감소시켜 정확도 향상 

#### 

#### 종류

• 보팅(Voting)

• 배깅(Bagging)

• 부스팅(Boosting)

• 랜덤포레스트(Random Forest)

• 스태킹(Stacked Generalization)



#### 보팅

- 하드 보팅 : 다수결 방식

- 소프트 보팅 : 개별 예측 확률들의 평균을 최종 확률로 정함



#### 배깅(Bagging)

- 개별 모델이 서로 다른 샘플링 데이터 활용

- 최종 예측확률은 보팅을 따름
  
  - 범주형 : 하드보팅
  
  - 연속형 : 소프트 보팅



#### 부스팅(Boosting)

- 가중치를 활용해 약한 모델을 강하게 만듦

- 배깅은 결정 트리 1과 결정 트리 2가 서로 독립적으로 결과를 예측

- 부스팅은 모델간 협력이 이루어짐

- XGBoost, LightGBM, CatBoost 등에 적극 반영 ▶ 2023년 기준, 분석가들이 가장 많이 사용하는 주요 알고리즘



#### 랜덤포레스트

- 결정 트리를 배깅 방식으로 결합한 모델
